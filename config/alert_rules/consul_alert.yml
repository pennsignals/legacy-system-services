groups:
- name: consul_alerts
  rules:  
  
  - alert: ContainerKilled
    expr: time() - container_last_seen > 60
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Container killed (instance {{ $labels.instance }})"
      description: "A container has disappeared\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ContainerCpuUsage
    expr: (sum(rate(container_cpu_usage_seconds_total[3m])) BY (instance, name) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Container CPU usage (instance {{ $labels.instance }})"
      description: "Container CPU usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ContainerMemoryUsage
    expr: (sum(container_memory_usage_bytes) BY (instance, name) / sum(container_spec_memory_limit_bytes) BY (instance, name) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Container Memory usage (instance {{ $labels.instance }})"
      description: "Container Memory usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ContainerVolumeUsage
    expr: (1 - (sum(container_fs_inodes_free) BY (instance) / sum(container_fs_inodes_total) BY (instance)) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Container Volume usage (instance {{ $labels.instance }})"
      description: "Container Volume usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ContainerVolumeIoUsage
    expr: (sum(container_fs_io_current) BY (instance, name) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Container Volume IO usage (instance {{ $labels.instance }})"
      description: "Container Volume IO usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ContainerHighThrottleRate
    expr: rate(container_cpu_cfs_throttled_seconds_total[3m]) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Container high throttle rate (instance {{ $labels.instance }})"
      description: "Container is being throttled\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: PgbouncerActiveConnectinos
    expr: pgbouncer_pools_server_active_connections > 200
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PGBouncer active connectinos (instance {{ $labels.instance }})"
      description: "PGBouncer pools are filling up\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: PgbouncerErrors
    expr: increase(pgbouncer_errors_count{errmsg!="server conn crashed?"}[5m]) > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PGBouncer errors (instance {{ $labels.instance }})"
      description: "PGBouncer is logging errors. This may be due to a a server restart or an admin typing commands at the pgbouncer console.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: PgbouncerMaxConnections
    expr: rate(pgbouncer_errors_count{errmsg="no more connections allowed (max_client_conn)"}[1m]) > 0
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "PGBouncer max connections (instance {{ $labels.instance }})"
      description: "The number of PGBouncer client connections has reached max_client_conn.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyDown
    expr: haproxy_up = 0
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy down (instance {{ $labels.instance }})"
      description: "HAProxy down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyHighHttp4xxErrorRateBackend
    expr: sum by (backend) irate(haproxy_server_http_responses_total{code="4xx"}[1m]) / sum by (backend) irate(haproxy_server_http_responses_total{}[1m]) * 100 > 5
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy high HTTP 4xx error rate backend (instance {{ $labels.instance }})"
      description: "Too many HTTP requests with status 4xx (> 5%) on backend {{ $labels.fqdn }}/{{ $labels.backend }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyHighHttp4xxErrorRateBackend
    expr: sum by (backend) irate(haproxy_server_http_responses_total{code="5xx"}[1m]) / sum by (backend) irate(haproxy_server_http_responses_total{}[1m]) * 100 > 5
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy high HTTP 4xx error rate backend (instance {{ $labels.instance }})"
      description: "Too many HTTP requests with status 5xx (> 5%) on backend {{ $labels.fqdn }}/{{ $labels.backend }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyHighHttp4xxErrorRateServer
    expr: sum by (server) irate(haproxy_server_http_responses_total{code="4xx"}[1m]) / sum by (backend) irate(haproxy_server_http_responses_total{}[1m]) * 100 > 5
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy high HTTP 4xx error rate server (instance {{ $labels.instance }})"
      description: "Too many HTTP requests with status 4xx (> 5%) on server {{ $labels.server }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyHighHttp5xxErrorRateServer
    expr: sum by (server) irate(haproxy_server_http_responses_total{code="5xx"}[1m]) / sum by (backend) irate(haproxy_server_http_responses_total{}[1m]) * 100 > 5
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy high HTTP 5xx error rate server (instance {{ $labels.instance }})"
      description: "Too many HTTP requests with status 5xx (> 5%) on server {{ $labels.server }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyBackendConnectionErrors
    expr: sum by (backend) rate(haproxy_backend_connection_errors_total[1m]) * 100 > 5
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy backend connection errors (instance {{ $labels.instance }})"
      description: "Too many connection errors to {{ $labels.fqdn }}/{{ $labels.backend }} backend (> 5%). Request throughput may be to high.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyServerResponseErrors
    expr: sum by (server) rate(haproxy_server_response_errors_total[1m]) * 100 > 5
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy server response errors (instance {{ $labels.instance }})"
      description: "Too many response errors to {{ $labels.server }} server (> 5%).\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyServerConnectionErrors
    expr: sum by (server) rate(haproxy_server_connection_errors_total[1m]) * 100 > 5
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy server connection errors (instance {{ $labels.instance }})"
      description: "Too many connection errors to {{ $labels.server }} server (> 5%). Request throughput may be to high.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyBackendMaxActiveSession
    expr: avg_over_time((sum by (backend) (haproxy_server_max_sessions) / sum by (backend) (haproxy_server_limit_sessions)) [2m]) * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "HAProxy backend max active session (instance {{ $labels.instance }})"
      description: "HAproxy backend {{ $labels.fqdn }}/{{ $labels.backend }} is reaching session limit (> 80%).\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyPendingRequests
    expr: sum by (backend) haproxy_backend_current_queue > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "HAProxy pending requests (instance {{ $labels.instance }})"
      description: "Some HAProxy requests are pending on {{ $labels.fqdn }}/{{ $labels.backend }} backend\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyHttpSlowingDown
    expr: avg by (backend) (haproxy_backend_http_total_time_average_seconds) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "HAProxy HTTP slowing down (instance {{ $labels.instance }})"
      description: "Average request time is increasing\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyRetryHigh
    expr: rate(sum by (backend) (haproxy_backend_retry_warnings_total)) > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "HAProxy retry high (instance {{ $labels.instance }})"
      description: "High rate of retry on {{ $labels.fqdn }}/{{ $labels.backend }} backend\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyBackendDown
    expr: haproxy_backend_up = 0
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy backend down (instance {{ $labels.instance }})"
      description: "HAProxy backend is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyServerDown
    expr: haproxy_server_up = 0
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "HAProxy server down (instance {{ $labels.instance }})"
      description: "HAProxy server is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyFrontendSecurityBlockedRequests
    expr: rate(sum by (frontend) (haproxy_frontend_requests_denied_total)) > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "HAProxy frontend security blocked requests (instance {{ $labels.instance }})"
      description: "HAProxy is blocking requests for security reason\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: HaproxyServerHealthcheckFailure
    expr: increase(haproxy_server_check_failures_total) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "HAProxy server healthcheck failure (instance {{ $labels.instance }})"
      description: "Some server healthcheck are failing on {{ $labels.server }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: SidekiqQueueSize
    expr: sidekiq_queue_size{} > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Sidekiq queue size (instance {{ $labels.instance }})"
      description: "Sidekiq queue {{ $labels.name }} is growing\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: SidekiqSchedulingLatencyTooHigh
    expr: max(sidekiq_queue_latency) > 120
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Sidekiq scheduling latency too high (instance {{ $labels.instance }})"
      description: "Sidekiq jobs are taking more than 2 minutes to be picked up. Users may be seeing delays in background processing.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ConsulServiceHealthcheckFailed
    expr: consul_catalog_service_node_healthy == 0
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Consul service healthcheck failed (instance {{ $labels.instance }})"
      description: "Service: `{{ $labels.service_name }}` Healthcheck: `{{ $labels.service_id }}`\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ConsulMissingMasterNode
    expr: consul_raft_peers < 3
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Consul missing master node (instance {{ $labels.instance }})"
      description: "Numbers of consul raft peers should be 3, in order to preserve quorum.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: ConsulAgentUnhealthy
    expr: consul_health_node_status{status="critical"} == 1
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Consul agent unhealthy (instance {{ $labels.instance }})"
      description: "A Consul agent is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - alert: OpenebsUsedPoolCapacity
    expr: (openebs_used_pool_capacity_percent) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "OpenEBS used pool capacity (instance {{ $labels.instance }})"
      description: "OpenEBS Pool use more than 80% of his capacity\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

